{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FA23 DAAN570 - Deep Learning\n",
    "#### Project: UAV Control using CNN and ViT Gesture Recognition\n",
    "#### Model: Model 10 (vit-base-patch16-224-in21k)\n",
    "##### Students: Aureo Zanon and Johnny Zielinski (Team 15)\n",
    "##### Date: December 6th, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pathlib\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import (Layer, GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, \n",
    "                                     MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input, \n",
    "                                     Embedding, Permute, Dropout, RandomFlip, RandomRotation, LayerNormalization, \n",
    "                                     MultiHeadAttention, RandomContrast, Rescaling, Resizing, Reshape, LeakyReLU)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD\n",
    "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
    "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "from tensorflow.train import BytesList, FloatList, Int64List, Example, Features, Feature\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16, VGG19, EfficientNetB0\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,  MobileViTForImageClassification, MobileViTModel, MobileViTConfig, MobileViTFeatureExtractor, TFViTModel\n",
    "import requests\n",
    "from keras.applications import imagenet_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT Model Configuration settings\n",
    "\n",
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IM_SIZE\": 224,\n",
    "    \"LEARNING_RATE\": 1e-3,\n",
    "    \"N_EPOCHS\": 20,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"REGULARIZATION_RATE\": 0.0,\n",
    "    \"N_FILTERS\": 6,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 1,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_DENSE_1\": 1024,\n",
    "    \"N_DENSE_2\": 128,\n",
    "    \"NUM_CLASSES\": 9,\n",
    "    \"PATCH_SIZE\": 32,\n",
    "    \"PROJ_DIM\": 768,\n",
    "    \"CLASS_NAMES\": [\"up\", \"pointer_r\", \"pointer_l\", \n",
    "    \"pointer_f\", \"palm_u\", \"palm_m\", \"palm\", \"ele\", \"down\"],\n",
    "    \n",
    "}\n",
    "\n",
    "# \"NUM_CLASSES_ORIGINAL: 12,\n",
    "# \"CLASS_NAMES_ORIGINAL\": [\"up\", \"pointer_r\", \"pointer_l\", \n",
    "# \"pointer_f\", \"pointer_b\", \"pointer\", \"palm_u\", \n",
    "# \"palm_o\", \"palm_m\", \"palm\", \"ele\", \"down\"],\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the base path to the datasets\n",
    "\n",
    "base_path = '//Users//aureozanon//Documents//DAAN570//Project//HG_Data//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=20, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest',\n",
    "                                   validation_split=0.1)  # 10% for validation due to the large dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Training and Validation generators\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_path, \n",
    "    target_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]), \n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"], \n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=74,\n",
    "    )\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_path, \n",
    "    target_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]), \n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"], \n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=74,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "### tf.keras.layer augment\n",
    "augment_layers = tf.keras.Sequential([\n",
    "  RandomRotation(factor = (-0.025, 0.025)),\n",
    "  RandomFlip(mode='horizontal',),\n",
    "  RandomContrast(factor=0.1),\n",
    "])\n",
    "\n",
    "def augment_layer(image, label):\n",
    "  return augment_layers(image, training = True), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutmix Augmentation Exploration\n",
    "\n",
    "def box(lamda):\n",
    "\n",
    "  r_x = tf.cast(tfp.distributions.Uniform(0, CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype = tf.int32)\n",
    "  r_y = tf.cast(tfp.distributions.Uniform(0, CONFIGURATION[\"IM_SIZE\"]).sample(1)[0], dtype = tf.int32)\n",
    "\n",
    "  r_w = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
    "  r_h = tf.cast(CONFIGURATION[\"IM_SIZE\"]*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
    "\n",
    "  r_x = tf.clip_by_value(r_x - r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
    "  r_y = tf.clip_by_value(r_y - r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
    "\n",
    "  x_b_r = tf.clip_by_value(r_x + r_w//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
    "  y_b_r = tf.clip_by_value(r_y + r_h//2, 0, CONFIGURATION[\"IM_SIZE\"])\n",
    "\n",
    "  r_w = x_b_r - r_x\n",
    "  if(r_w == 0):\n",
    "    r_w  = 1\n",
    "\n",
    "  r_h = y_b_r - r_y\n",
    "  if(r_h == 0):\n",
    "    r_h = 1\n",
    "\n",
    "  return r_y, r_x, r_h, r_w\n",
    "\n",
    "\n",
    "\n",
    "def cutmix(train_dataset_1, train_dataset_2):\n",
    "  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n",
    "\n",
    "  lamda = tfp.distributions.Beta(2,2)\n",
    "  lamda = lamda.sample(1)[0]\n",
    "\n",
    "  r_y, r_x, r_h, r_w = box(lamda)\n",
    "  crop_2 = tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n",
    "  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])\n",
    "\n",
    "  crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
    "  pad_1 = tf.image.pad_to_bounding_box(crop_1, r_y, r_x, CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])\n",
    "\n",
    "  image = image_1 - pad_1 + pad_2\n",
    "\n",
    "  lamda = tf.cast(1- (r_w*r_h)/(CONFIGURATION[\"IM_SIZE\"]*CONFIGURATION[\"IM_SIZE\"]), dtype = tf.float32)\n",
    "  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n",
    "\n",
    "  return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing and rescaling\n",
    "resize_rescale_hf = tf.keras.Sequential([\n",
    "       Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
    "       Rescaling(1./255),\n",
    "       Permute((3,1,2))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the base model\n",
    "\n",
    "from transformers import ViTFeatureExtractor, TFViTModel\n",
    "\n",
    "\n",
    "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# feature_extractor = MobileViTFeatureExtractor.from_pretrained('Matthijs/mobilevit-small')\n",
    "# base_model = MobileViTForImageClassification.from_pretrained('Matthijs/mobilevit-small')\n",
    "\n",
    "inputs = Input(shape = (CONFIGURATION[\"IM_SIZE\"],CONFIGURATION[\"IM_SIZE\"],3))\n",
    "x = resize_rescale_hf(inputs)\n",
    "x = base_model.vit(x)[0][:,0,:]\n",
    "#print(x)\n",
    "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')(x)\n",
    "\n",
    "hf_model = tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Attention Maps\n",
    "\n",
    "from transformers import ViTFeatureExtractor, TFViTModel, ViTConfig\n",
    "\n",
    "configuration = ViTConfig()\n",
    "configuration.output_attentions = True\n",
    "\n",
    "base_model = TFViTModel.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\",\n",
    "    config = configuration,\n",
    "    )\n",
    "inputs = Input(shape = (CONFIGURATION[\"IM_SIZE\"],CONFIGURATION[\"IM_SIZE\"],3))\n",
    "x = resize_rescale_hf(inputs)\n",
    "x = base_model.vit(x)['attentions']\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings to train the HF model\n",
    "\n",
    "loss_function = CategoricalCrossentropy()\n",
    "#loss_function = SparseCategoricalCrossentropy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='ViT_Model9_AZ.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "# model compiled using the SGD optimizer, with learning rate of 0.001 and momentum of 0.9 (tried other combinations in previous models)\n",
    "# used the categorical_crossentropy loss and set accuracy as the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the new model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAAN570",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
